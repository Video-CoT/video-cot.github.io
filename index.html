<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought">
  <meta property="og:title" content="Video-CoT"/>
  <meta property="og:description" content="A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought"/>
  <meta property="og:url" content="https://github.com/Video-CoT/video-cot.github.io"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="Video-CoT">
  <meta name="twitter:description" content="A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Video Understanding, Spatiotemporal Alignment, Multimedia">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought</title>
  <link rel="icon" type="image/x-icon" href="static/images/Video-CoT.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://video-cot.github.io">Shuyi Zhang</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://video-cot.github.io">Xiaohuai Hao</a><sup>2,*</sup>,</span>
            <span class="author-block">
              <a href="https://video-cot.github.io">Yingbo Tang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://video-cot.github.io">Lingfeng Zhang</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://video-cot.github.io">Pengwei Wang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://video-cot.github.io">Zhongyuan Wang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://video-cot.github.io">Hongxuan Ma</a><sup>1,†</sup>,</span>
            <span class="author-block">
              <a href="https://video-cot.github.io">Shanghang Zhang</a><sup>4,†</sup>,</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institute of Automation, Chinese Academy of Sciences</span>
            <span class="author-block"><sup>2</sup>Beijing Academy of Artificial Intelligence (BAAI)</span>
            <span class="author-block"><sup>3</sup>Shenzhen International Graduate School, Tsinghua University</span>
            <span class="author-block"><sup>4</sup>State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University</span>
            <span class="author-block"><small><br><sup>*</sup>Co-first Authors</small></span>
            <span class="author-block"><small><br><sup>†</sup>Corresponding Author</small></span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/XXXX.XXXXX.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Supplementary</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/Video-CoT/video-cot.github.io/" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/Video-CoT/video-cot.github.io/" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="far fa-images"></i></span><span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <img src="./static/images/fig1_shuyi conv 1.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        In this work, we introduce <strong>Video-CoT</strong>, a groundbreaking dataset designed to enhance spatiotemporal understanding using <strong>Chain-of-Thought (CoT)</strong> methodologies, aiming to encourage further exploration in video reasoning area.
      </h2>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video content comprehension is essential for various applications, ranging from video analysis to interactive systems. Despite advancements in large-scale vision-language models (VLMs), these models often struggle to <strong>capture the nuanced, spatiotemporal details</strong> essential for thorough video analysis. 
          </p>
          <p>
            To address this gap, we introduce <strong>Video-CoT</strong>, a groundbreaking dataset designed to enhance spatiotemporal understanding using <strong>Chain-of-Thought (CoT)</strong> methodologies. <strong>Video-CoT</strong> contains 192,000 fine-grained spatiotemporal question-answer pairs and 23,000 high-quality CoT-annotated samples, providing a solid foundation for evaluating spatiotemporal understanding in video comprehension. Additionally, we provide a comprehensive benchmark for assessing these tasks, with each task featuring 750 images and tailored evaluation metrics.
          </p>
          <p>
            Our extensive experiments reveal that current VLMs face significant challenges in achieving satisfactory performance, highlighting the difficulties of effective spatiotemporal understanding. Overall, the Video-CoT dataset open new avenues for research in multimedia understanding and support future innovations in intelligent systems requiring advanced video analysis capabilities. By making these resources publicly available, we aim to encourage further exploration in this critical area. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Abstract -->

<section class="section hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset Samples</h2>
          <img src="./static/images/data_sample.png" alt="MY ALT TEXT"/>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset Construction</h2>
          <img src="./static/images/pipeline.png" alt="MY ALT TEXT"/>
          <div class="content has-text-justified">
            Pipeline for constructing the Video-CoT dataset. We first collect video data with dense annotations, and then generate problem-solving thinking processes and answers using Qwen2.5-VL 72B model. After eliminating data with low-accuracy answers and disorganized thinking processes, the remaining data then served as CoT data.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2>
          <img src="./static/images/table1.png" alt="MY ALT TEXT"/>
          <div class="content has-text-justified">
            Comparison results of various VLMs on nuanced spatiotemporal understanding tasks.
          <img src="./static/images/table2.png" alt="MY ALT TEXT"/>
          <div class="content has-text-justified">
            Ablation experiments on Ans-SFT and CoT-SFT.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
        
      

<!-- You can now continue with your video, carousel, BibTeX, etc. -->

 
